# 12月23日汇报

## 残差学习 + 自注意力机制 + 加权损失的多步轨迹预测网络

为提升多变量轨迹预测任务中**位置预测精度**并增强模型对关键历史片段的表征能力，本文在传统 LSTM 类时序回归网络基础上，从**预测目标构造、网络结构设计与优化目标函数**三个层面进行改进，形成“残差预测网络 + 注意力机制 + 加权损失”的端到端学习框架。模型输入为长度为 \(T\) 的历史序列 \(\mathbf{X}_{1:T}\)，每个时刻包含 5 维特征（如纬度、经度、速度、航向等），输出为两个未来时刻（例如 \(t+\Delta_1\)、\(t+\Delta_2\)）的 5 维状态，共 10 维预测量。

---

### 1. 预测目标重构：由绝对值回归转为残差学习（Residual Learning）

在基线模型中，网络通常直接回归未来状态的**绝对值**：
\[
\hat{\mathbf{y}}_{t+\Delta} = f_\theta(\mathbf{X}_{1:T})
\]
该设置会使网络同时承担“坐标/状态的全局尺度拟合”与“局部运动变化建模”两类任务，尤其在地理坐标或多源特征尺度不一致时，容易导致优化困难或泛化不稳定。

为此，本文采用残差学习思想，将预测目标从绝对值重构为相对于当前序列末端状态的**增量（残差）**。令输入序列末端观测为：
\[
\mathbf{x}_t = \mathbf{X}_{1:T}(:,T)
\]
对应两个预测时刻的真值分别为 \(\mathbf{y}_{t+\Delta_1}\)、\(\mathbf{y}_{t+\Delta_2}\)。则残差监督信号定义为：
\[
\mathbf{r}_{t+\Delta_k} = \mathbf{y}_{t+\Delta_k} - \mathbf{x}_t,\quad k\in\{1,2\}
\]
网络训练阶段输出残差：
\[
\hat{\mathbf{r}}_{t+\Delta_k} = f_\theta(\mathbf{X}_{1:T})
\]
推理阶段再将残差与末端状态相加，恢复绝对预测：
\[
\hat{\mathbf{y}}_{t+\Delta_k} = \mathbf{x}_t + \hat{\mathbf{r}}_{t+\Delta_k}
\]

该策略将学习重点聚焦于“短时动力学变化”，能够降低输出空间的动态范围、缓解非平稳性影响，并在多变量回归中提升收敛稳定性。对轨迹预测而言，残差形式也更贴近“位移/速度变化”的物理意义，使模型更易捕捉局部运动模式。

---

### 2. 网络结构增强：双向 LSTM 深层时序编码 + 自注意力模块

#### 2.1 深层双向 LSTM 编码器
基线网络常采用单层或浅层 LSTM（输出 last hidden state）作为时序编码。本文构建更强的时序表示学习模块：首先使用两层双向 LSTM（BiLSTM）并设置 `OutputMode="sequence"` 输出每个时间步的隐状态序列：
\[
\mathbf{H} = [\mathbf{h}_1,\mathbf{h}_2,\ldots,\mathbf{h}_T]
\]
双向结构能够同时利用过去与未来方向的上下文信息（在离线训练的序列表示学习中常用于增强特征提取能力），深层堆叠则提升对复杂时序依赖的建模容量。每层 BiLSTM 后加入 Dropout，用于抑制过拟合、提升泛化能力。

#### 2.2 自注意力机制（Self-Attention）建模长程依赖与关键片段
在获得隐状态序列 \(\mathbf{H}\) 后，本文进一步引入多头自注意力机制对时间维度进行重加权聚合。自注意力通过计算不同时间步之间的相关性，学习得到对预测更有贡献的历史片段权重，从而实现“选择性记忆”。形式化地，注意力模块可写为：
\[
\mathrm{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V}) = \mathrm{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d}}\right)\mathbf{V}
\]
其中 \(\mathbf{Q},\mathbf{K},\mathbf{V}\) 由 \(\mathbf{H}\) 线性映射得到，多头机制并行学习多个子空间的相关性模式。与仅依赖 LSTM 最后时刻隐状态的做法相比，自注意力能够显式捕捉长程依赖并突出关键事件（如转向、变速等），对轨迹预测中的非匀速运动段尤其有效。

在实现上，本文在 MATLAB 版本支持（2020 及以上）时使用 `selfAttentionLayer`（多头数为 4，注意力特征维为 64）作为序列特征增强模块；若版本不支持，则采用额外的 BiLSTM（`OutputMode="last"`）作为替代，以保证整体网络在不同环境下的可训练性。

#### 2.3 回归头（Regression Head）加深与非线性增强
注意力/时序编码后，本文采用 `LSTM(OutputMode="last")` 将增强后的序列表征压缩为固定长度向量，再通过两层全连接网络进行非线性映射：
- FC(256) + LeakyReLU + Dropout  
- FC(128) + LeakyReLU  
最终通过 FC 输出 10 维残差预测（两个未来时刻 × 5 维特征）。LeakyReLU 用于缓解 ReLU 在负半轴“神经元死亡”的问题，使回归任务中梯度传播更稳定。

---

### 3. 优化目标改进：引入加权回归损失以强化位置精度

轨迹预测任务通常以地理位置误差为核心指标，而常规回归层（MSE）对所有输出维度等权处理，可能导致模型在训练时过度关注易优化的维度（如速度/航向），而位置维度精度提升不足。为此，本文设计加权回归损失，对不同输出维度赋予不同权重，从而显式引导网络优化重点。

设输出维度为 \(d\in\{1,\ldots,10\}\)，对应两个未来时刻的 5 维状态。加权均方误差定义为：
\[
\mathcal{L} = \frac{1}{N}\sum_{i=1}^{N}\sum_{d=1}^{10} w_d\left(y_{i,d}-\hat{y}_{i,d}\right)^2
\]
其中 \(w_d\) 为维度权重。本文将经纬度（位置）相关维度权重提高（如 Lat/Lon 权重 3.0），速度权重设为 1.0，航向权重设为 1.5，以提升位置预测在总损失中的贡献度。该策略使模型在训练阶段即与最终评价目标（位置误差）对齐，有助于在相同模型容量下获得更优的空间精度表现。

---

### 4. 多步联合预测：一次输出两个未来时刻，避免滚动误差累积

与逐步滚动预测（一步预测再作为下一步输入）不同，本文采用**多时间步联合回归**：网络直接输出 \((t+\Delta_1)\) 与 \((t+\Delta_2)\) 两个未来点的残差，共 10 维。该方式能够共享同一套时序表征，并降低滚动预测带来的误差传播风险；同时，通过统一损失函数对两个时刻同时约束，有利于提升短期与稍远期预测的一致性。

---

### 5. 训练策略与工程化设置（用于复现实验的描述）

训练采用 Adam 优化器，设置分段学习率衰减策略（piecewise schedule），并引入 L2 正则、Dropout 与梯度裁剪以提升稳定性与泛化能力。训练过程中使用验证集监控并设置早停（ValidationPatience），以避免过拟合。执行环境根据可用 GPU 自动选择加速训练。模型训练完成后保存网络权重及归一化参数（均值 \(\mu\) 与标准差 \(\sigma\)），以保证推理阶段能够一致地进行反归一化处理与结果复现。

---

### 6. 评价方式：地理距离误差（Haversine）作为位置精度指标

为贴合实际轨迹预测需求，本文在反归一化后将预测经纬度转换为球面距离误差，采用 Haversine 公式计算预测点与真值点之间的地理距离，并统计 RMSE、均值、中位数以及小于阈值（如 50m）比例等指标，从而从空间意义上评估位置预测性能。该评价方式比在归一化空间直接做 MSE 更具物理可解释性，也更能反映模型在真实场景中的定位误差水平。

---

##（可选）一句话总结（放在段末）
综上，本文通过**残差监督**降低学习难度，通过**自注意力机制**增强关键历史片段建模能力，并通过**加权损失函数**将优化重点聚焦于位置精度，同时采用**多步联合输出**提升多时刻预测的稳定性与一致性。

---

如果你希望这段文字与“基础模型”**逐条对齐**（例如：基线是 *单层 LSTM(last)+FC+regressionLayer* 还是 *BiLSTM+MSE*），把你的基线结构（层数、是否残差、是否注意力、损失函数）发我，我可以把上述内容改成论文里常用的 **“Baseline / Ours” 对照式描述 + 消融实验表述模板**。